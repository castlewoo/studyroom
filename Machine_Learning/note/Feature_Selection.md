# 특성 공학

### 특성공간 차원축소의 필요성
- 모델의 해석력 향상
- 모델 훈련시간의 단축
- 차원의 저주 방지
- 과적합에 의한 일반화 오차를 줄여 성능 향상

### 특성공학의 방법론은 크게 특성 선택과 특성 추출 방법으로 구분
---
## 특성 선택 방법론
    - 주어진 특성 변수들 가운데 가장 좋은 특성변수의 조합만 선택
    - 불필요한 특성 변수를 제거
    - Filtering, Wrapper, Embedded 방식

1. Filter 방식 : 각 특성변수를 독립적인 평가함수로 평가
   - 각 특성변수 x와 목표변수 y와의 연관성 측정한 뒤, 목표변수를 잘 설명할 수 있는 특성 변수 선택
   - x와 y의 1:1 관계로만 연관성 판당
   - t-test, chi-square test, information gain등의 지표 활용

2. Wrapper 방식 : 학습 알고리즘 이용
   - 훈련된 모델의 예측력을 평가. 결과를 비교하여 최적화된 특성변수의 조합을 찾는다
   - 특성변수의 조합이 바뀔 때 마다 모델을 학습
   - 특성변수에 중복된 정보가 많은 경우 효과적으로 제거
   - 순차탐색범인 forward selection, backward selection, stepwise selection 등

3. Embedded 방식 : 학습 알고리즘 자체에 feature selection을 포함
    - 학습 과정에서 최적화된 변수를 선택
    - 특성변수 파라미터에 규제를 가함
    - Ridge, Lasso, Elastic net 등

## 특성 추출 방법론

1. 주성분 분석(PCA)
    - 서로 연관되어 있는 변수들이 전체적으로 가지고 있는 정보들을 최대한 확보하는 적은 수의 새로운 변수를 생성하는 방법
    - 자료에서 변동이 큰 축을 탐색
    - 정보의 손실을 초소화하면서 차원 축소
    - 서로 상관이 없거나 독립적인 새로운 변수인 주성분을 통해 데이터의 해석을 용이

2. 특성값분해(SVD)