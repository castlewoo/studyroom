spark
하둡의 맵리듀스라는 데이터 처리(저장, 추출, 변환 등) 기능을 프레임으로 구현한 오픈소스
하둡 설치 후 spark를 얹어서 사용
필요환경
1. 파이썬
- 파이썬 설치 위치 확인 : which python3
2. 주피터 노트북
- pyspark를 사용하게됨
3. 자바 
4. hadoop
5. 스파크
6. pyspark

- 스파크 설치
0. 하둡이 없는 버전의 스파크 설치
#ref : https://www.apache.org/dyn/closer.lua/spark
- 다운 버전은 확인하고 다운 받아야 함
# 이전 버전
wget https://archive.apache.org/dist/spark/spark-3.2.2/spark-3.2.2-bin-without-hadoop.tgz
tar -xvzf spark-3.2.2-bin-without-hadoop.tgz 압축 풀기
ln -s spark-3.2.2-bin-without-hadoop spark 바로가기 생성

# 환경변수 설정
# spark 어느 위치에서나 실행가능하도록 설정(path)
cd
vim ~/.bashrc
export SPARK_HOME=/root/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
esd :wq
# 설정한 환경변수 시스템 적용
source ~/.bashrc

# 스파크 설정
- spark와 hadoop 연결 위한 설정
- spark 설정 파일 : spark-env.sh
- 템플릿 제공하므로 복사 해서 사용함
- cd $SPARK_HOME/conf
- cd spark/conf
- cp.spark-env.sh.template spark-env.sh

- vim spark-env.sh
# spark 실행할 때 hadoop 위치 지정 - spark 소스코드가 hadoop base로 build(실행상태) 됨
export SPARK_DIST_CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath)
# spark load data 저장소 hadoop 지정
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

- pyspark 설치
# --no-cache-dir : cache 사용하지 말것(ram 부족한 경우)
- pip install pyspark==3.2.4 --no-cache-dir
- pyspark을 사용하기 위한 환경변수 설정 (파이썬 위치 등록)
- 파이썬3 위치 확인
- 파이썬 위치를 spark-env.sh에 등록
cd
which python3

cd spark/conf
vim spark-env.sh
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3

- spark 실행 확인
- spark 실행 전 hadoop 실행해야 함
start-dfs.sh
start-yarn.sh

pyspark --master yarn

- pyspark의 기본 리소스 매니저 yarn으로 설정
- spark-defaults.con 파일에서 진행
cd
cd spark/conf
cp spark-defaults.conf.template spark-defaults.conf
vim spark-defaults.conf
spark.master                    yarn